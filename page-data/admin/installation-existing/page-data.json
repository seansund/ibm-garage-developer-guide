{"componentChunkName":"component---src-pages-admin-installation-existing-index-mdx","path":"/admin/installation-existing/","result":{"pageContext":{"frontmatter":{"title":"Install into existing cluster"},"relativePagePath":"/admin/installation-existing/index.mdx","titleType":"page","MdxNode":{"id":"d34acf5d-c430-5af0-9344-fef7a4761e76","children":[],"parent":"0856033f-a5ad-5cf7-af3c-3b57a61824bc","internal":{"content":"---\ntitle: Install into existing cluster\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n\n<InlineNotification>\n\n**Note**: An **environment administrator** performs the steps on this page. See [Plan Installation > Roles](/admin/plan-installation#roles) for the overview of the roles involved.\n\n</InlineNotification>\n\n\n<Tabs>\n\n<Tab label=\"Prerequisites\">\n\n<PageDescription>\n\nPrepare to run Terraform to install the Development Tools into an existing IBM Cloud managed cluster\n\n</PageDescription>\n\nThe <Globals name=\"env\" /> is installed by an environment administrator, who will run the scripts to create the environment in an IBM Cloud account. The scripts will run as the environment administrator's user, so the user needs the permissions described in [Plan Installation](/admin/plan-installation). These instructions explain how to configure and run the Terraform [infrastructure-as-code](/admin/terraform) (IasC) scripts to create the <Globals name=\"env\" />.\n\n<InlineNotification>\n\n**Note**: The Terraform scripts will clean up the cluster to remove any existing resources that overlap with the resources\nprovisioned by the scripts. You will need to remember to remove any IBM Cloud services that were previously\ncreated.\n\nUse the helper script `terraform/scripts/destroy-service.sh` to help automate the deletion You can also run it\nto remove all the services beside **PostgreSQL** `./destroy-services.sh postgres`\n\n</InlineNotification>\n\n### Prepare local system\n\nTo run the scripts, install the client tools needed to use the <Globals name=\"env\" />:\n- Install the [Prerequisites](/getting-started/prereqs)\n\n### Test login\n\nOptional: The environment administrator must be able to log into IBM Cloud, specifying the account, region, and resource group. That is effectively what the scripts will do when they run, using the environment administrator's API key.\n\n- Log in to IBM Cloud using the CLI\n    ```bash\n    ibmcloud login -a cloud.ibm.com -r <region> -g <resource-group>\n    ```\n\n### Confirm resources\n\nOptional: To help confirm that the account has the resources the scripts will require (see [Plan Installation](/admin/plan-installation)), the environment administrator may log into the account view the resources:\n- [Existing VLANs](https://cloud.ibm.com/classic/network/vlans) -- Verify that the two VLANs specified are in the list and that they are defined in the proper data center\n- [Existing resource groups](https://cloud.ibm.com/account/resource-groups) -- Having logged in using the environment's resource group, it should certainly be in the list\n- [Existing Kubernetes clusters](https://cloud.ibm.com/kubernetes/clusters) -- See what clusters (Kubernetes and OpenShift) already exist in the resource group and confirm one does not already exist for the environment you want to create\n\n### Confirm permissions\n\nOptional: To help confirm that the scripts will have the permissions they'll require (see the administrators access group in [Plan Installation](/admin/plan-installation)), the environment administrator may log into the account and test creating a couple of resources:\n- [Create a cluster](https://cloud.ibm.com/kubernetes/catalog/cluster/create) -- Make it single-zone, and specify the proper data center and resource group\n- Create a namespace in the image registry\n- [Create an instance of Cloudant](https://cloud.ibm.com/catalog/services/cloudant) -- Select a paid plan and specify the proper region and resource group\n- [Create an instance of Sysdig](https://cloud.ibm.com/observe/monitoring/create) -- Select a paid plan and specify the proper region and resource group\n\nAs long as the user can create these resources successfully, the scripts should be able to as well. You may delete these test resources.\n\n</Tab>\n\n\n<Tab label=\"Download\">\n\n<PageDescription>\n\nObtain the Terraform infrastructure-as-code (IasC) scripts that will install the tools into your IBM Cloud managed cluster\n\n</PageDescription>\n\n- Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem\n    ```bash\n    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git\n    ```\n\n- Switch to the cloned directory\n    ```bash\n    cd ibm-garage-iteration-zero\n    ```\n\n</Tab>\n\n<Tab label=\"API key\">\n\n<PageDescription>\n\nConfigure the key the CLI uses to create cloud services\n\n</PageDescription>\n\nThe installation scripts need an API key to authenticate to IBM Cloud.\nThe following steps create the required API key and put them into a properties file.\n\n- The `credentials.properties` property file looks like this:\n\n    ```bash\n    classic.username=<CLASSIC_USERNAME>\n    classic.api.key=<CLASSIC_API_KEY>\n    ibmcloud.api.key=<IBMCLOUD_API_KEY>\n    ```\n\nSince the cluster already exists, we'll only need to set the `ibmcloud.api.key` property.\n\n### Step A. Create the credentials.properties file\n\n- Inside the `ibm-garage-iteration-zero` folder, copy `credentials.template` to a file named `credentials.properties`.\n\n  ```bash\n  cd ibm-garage-iteration-zero\n  cp credentials.template credentials.properties\n  ```\n\n  The `credentials.properties` file is already listed in the `.gitignore` file so that the file containing\nyour private API key will not be pushed to the server repository.\n\n### Step B. Set the IBM Cloud API key\n\n- Follow these instructions to create an IBM Cloud API key:\n\n    - [Managing user API keys](https://cloud.ibm.com/docs/iam?topic=iam-userapikey \"Managing user API keys\")\n\n- Edit the `credentials.properties` file to set the value for the `ibmcloud.api.key` property.\n\n</Tab>\n\n<Tab label=\"Configuration\">\n\n<PageDescription>\n\nConfigure the properties describing the environment\n\n</PageDescription>\n\nThe settings for installing the Developer Tools go in a single property file\nin the `./terraform/settings` directory:\n- `environment.tfvars` -- Properties for installing the Developer Tools\n\nFollow the instructions below to set the existing cluster's details in the `environment.tfvars` file.\n\n### Environment variables\n\nThe `environment.tfvars` properties will be used to install the Development Tools.\n\n- Typical values look like this:\n\n    ```bash\n    # The type of cluster that will be created/used (kubernetes or openshift)\n    cluster_type=\"openshift\"\n    # Flag indicating if we are using an existing cluster or creating a new one\n    cluster_exists=\"true\"\n\n    # The prefix that should be applied to the cluster name and service names (if not provided\n    # explicitly). If not provided then the resource_group_name will be used as the prefix.\n    #name_prefix=\"<name prefix for cluster and services>\"\n    #name_prefix=\"garage-cloud-surge\"\n\n    # The cluster name can be provided (particularly if using an existing cluster). The value\n    # for cluster name used by the scripts will be set in the following order of presidence:\n    # - \"${cluster_name}\"\n    # - \"${name_prefix}-cluster\"\n    # - \"${resource_group_name}-cluster\"\n    #cluster_name=\"<cluster name>\"\n    cluster_name=\"garage-cloud-surge-ocp-cluster\"\n\n    resource_group_name=\"garage-cloud-surge\"\n    vlan_region=\"us-south\"\n    ```\n\n<p/>\n\nSet them based on the existing cluster:\n- `resource_group_name` -- The existing cluster's resource group\n- `vlan_region` -- The existing cluster's region\n- `cluster_exists` -- Set to `true` for an existing cluster\n- `cluster_type` -- Specify the existing cluster's type\n    - **kubernetes** -- Kubernetes\n    - **openshift** -- OpenShift v3\n    - **ocp3** -- OpenShift v3\n    - **ocp4** -- OpenShift v4\n    - **crc** -- CodeReady Containers\n- `cluster_name` -- The existing cluster's name\n- `registry_namespace` -- The namespace that will be created with the IBM Container Registry. If not provided\nthe value will default to the `resource_group_name`\n- `logdna_exists` -- \"true\" or \"false\" flag indicating that the logdna instance already exists and a new one \nshould not be provisioned\n- `logdna_name` -- The name of the logdna instance, particularly if one already exists. If not provided the name\nwill be generated as `{prefix-name || resource-group-name}-logdna`\n- `sysdig_exists` -- \"true\" or \"false\" flag indicating that the sysdig instance already exists and a new one \nshould not be provisioned\n- `sysdig_name` -- The name of the sysdig instance, particularly if one already exists. If not provided the name\nwill be generated as `{prefix-name || resource-group-name}-sysdig`\n\n<p/>\n\n<InlineNotification>\n\n**Note**: The values for `resource_group_name` and `cluster_name` can be found on the Resource List\npage in the IBM Cloud Console - https://cloud.ibm.com/resources\n\n</InlineNotification>\n\n<p/>\n\n![Resource list](/images/cloud.ibm.com-resources.png)\n\n<p/>\n\n</Tab>\n\n<Tab label=\"Run\">\n\n<PageDescription>\n\nRun the scripts that create the environment\n\n</PageDescription>\n\nHaving configured the `credentials.properties` and `environment.tfvars` properties files, we are now ready to kick off the installation.\n\nLaunch a [Developer Tools Docker container](https://github.com/ibm-garage-cloud/ibm-garage-cli-tools \"Cloud Garage Tools Docker image\").\n\n- Run the following command to run the Docker container:\n    ```bash\n    ./launch.sh\n    ```\n\n-  This will install the Cloud Garage Tools Docker image and exec shell into the running container. The container will\n    mount the filesystem's `./terraform/` directory as `/home/devops/src/`. Once the Docker container has started and\n    the script has exec shelled into it, you will see an IBM Garage banner. This will help you identify you are running\n    inside the Docker image that has just mounted your file system.\n\n    For more information on the **Developer Tools Image** see the following guide link below.\n\n- Create the <Globals name=\"env\" />\n\n    The supplied Terraform script is ready to run using the settings in the properties files.\n    You optionally can extend or modify the scripts and tailor them for your project's specific needs.\n\n<InlineNotification>\n\n**Note**: If you run this approach multiple times remember to delete any pre existing cloud services that were created previously\n\n</InlineNotification>\n\n- Run this script in the container:\n    ```bash\n    ./runTerraform.sh\n    ```\n\n    This script will create the <Globals name=\"env\" />.\n\n    If you executed the script previously for the current cluster configuration and the workspace directory still\n    exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if\n    you want to use the state from the previous run as a starting point to either add or remove configuration. Delete\n    the workspace if you want to start with a clean install of the Toolkit.\n\n    The script will verify some basic settings and prompt if you want to proceed. After you select **Y** (for yes), \n    the Terraform Apply process will begin to create the infrastructure and services for your environment.\n    \n    Both of the prompts can be skipped via command-line arguments passed to the script. To bypass the workspace\n    prompt, provided either `--delete` or `--keep` to either delete or keep the workspace, respectively. To\n    bypass the verification prompt provide `--auto-approve` and the script will automatically answer `Yes` to\n    the prompt and start the terraform process.\n\n    Installing the tools into an existing cluster takes about 30 minutes.\n\n    <InlineNotification kind=\"success\">\n\n    You should now have your <Globals name=\"env\" />\n        fully provisioned and configured. Enjoy!\n\n    </InlineNotification>\n\n### <Globals name=\"env\" />\n\nOnce the Terraform scripts have finished, you can see the resources that the scripts created in IBM Cloud:\n- In the IBM Cloud console, open the [Resource List](https://cloud.ibm.com/docs/overview?topic=overview-ui#dashboardview \"Managing resources in the resource list\")\n- On the Resource List page, filter by your Resource Group (e.g. `appdev-team`)\n- You should see these resources listed:\n    - **Clusters**: 1, either Kubernetes or OpenShift\n    - **Services**: 5 or so, such as PostgreSQL, LogDNA, etc.\n    - **Storage**: 1, an instance of Cloud Object Storage\n- Select the cluster and open the Kubernetes dashboard or OpenShift web console. You should see:\n    - Namespaces: `tools`, `dev`, `test`, and `staging`\n    - Deployments in the `tools` namespace: `developer-dashboard`, `jenkins`, etc.\n\nTo get started with code use the following guides:\n\n<AnchorLinks>\n  <AnchorLink to=\"/getting-started-day-1/deploy-app\">Deploy First App</AnchorLink>\n  <AnchorLink to=\"/tools\">Guides</AnchorLink>\n  <AnchorLink to=\"/tools/tools-image\">Developer Tools Image</AnchorLink>\n</AnchorLinks>\n\n### Possible issues\n\nIf you find that that the Terraform provisioning has failed, try re-running the `runTerraform.sh` script again.\nThe state will be saved and Terraform will try and apply the configuration to match the desired end state.\n\nIf you find that some of the services have failed to create in the time allocated, you can try this:\n- Manually delete the service instances in your resource group\n- Delete the `workspace` directory (which will remove any state that has been created by Terraform)\n- Re-run the `runTerraform.sh` script\n    ```bash\n    rm -rf workspace\n    ./runTerraform.sh\n    ```\n\n</Tab>\n\n</Tabs>\n\n","type":"Mdx","contentDigest":"e286b8865eac00f02c9947c685ad4bf1","counter":943,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Install into existing cluster"},"exports":{},"rawBody":"---\ntitle: Install into existing cluster\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n\n<InlineNotification>\n\n**Note**: An **environment administrator** performs the steps on this page. See [Plan Installation > Roles](/admin/plan-installation#roles) for the overview of the roles involved.\n\n</InlineNotification>\n\n\n<Tabs>\n\n<Tab label=\"Prerequisites\">\n\n<PageDescription>\n\nPrepare to run Terraform to install the Development Tools into an existing IBM Cloud managed cluster\n\n</PageDescription>\n\nThe <Globals name=\"env\" /> is installed by an environment administrator, who will run the scripts to create the environment in an IBM Cloud account. The scripts will run as the environment administrator's user, so the user needs the permissions described in [Plan Installation](/admin/plan-installation). These instructions explain how to configure and run the Terraform [infrastructure-as-code](/admin/terraform) (IasC) scripts to create the <Globals name=\"env\" />.\n\n<InlineNotification>\n\n**Note**: The Terraform scripts will clean up the cluster to remove any existing resources that overlap with the resources\nprovisioned by the scripts. You will need to remember to remove any IBM Cloud services that were previously\ncreated.\n\nUse the helper script `terraform/scripts/destroy-service.sh` to help automate the deletion You can also run it\nto remove all the services beside **PostgreSQL** `./destroy-services.sh postgres`\n\n</InlineNotification>\n\n### Prepare local system\n\nTo run the scripts, install the client tools needed to use the <Globals name=\"env\" />:\n- Install the [Prerequisites](/getting-started/prereqs)\n\n### Test login\n\nOptional: The environment administrator must be able to log into IBM Cloud, specifying the account, region, and resource group. That is effectively what the scripts will do when they run, using the environment administrator's API key.\n\n- Log in to IBM Cloud using the CLI\n    ```bash\n    ibmcloud login -a cloud.ibm.com -r <region> -g <resource-group>\n    ```\n\n### Confirm resources\n\nOptional: To help confirm that the account has the resources the scripts will require (see [Plan Installation](/admin/plan-installation)), the environment administrator may log into the account view the resources:\n- [Existing VLANs](https://cloud.ibm.com/classic/network/vlans) -- Verify that the two VLANs specified are in the list and that they are defined in the proper data center\n- [Existing resource groups](https://cloud.ibm.com/account/resource-groups) -- Having logged in using the environment's resource group, it should certainly be in the list\n- [Existing Kubernetes clusters](https://cloud.ibm.com/kubernetes/clusters) -- See what clusters (Kubernetes and OpenShift) already exist in the resource group and confirm one does not already exist for the environment you want to create\n\n### Confirm permissions\n\nOptional: To help confirm that the scripts will have the permissions they'll require (see the administrators access group in [Plan Installation](/admin/plan-installation)), the environment administrator may log into the account and test creating a couple of resources:\n- [Create a cluster](https://cloud.ibm.com/kubernetes/catalog/cluster/create) -- Make it single-zone, and specify the proper data center and resource group\n- Create a namespace in the image registry\n- [Create an instance of Cloudant](https://cloud.ibm.com/catalog/services/cloudant) -- Select a paid plan and specify the proper region and resource group\n- [Create an instance of Sysdig](https://cloud.ibm.com/observe/monitoring/create) -- Select a paid plan and specify the proper region and resource group\n\nAs long as the user can create these resources successfully, the scripts should be able to as well. You may delete these test resources.\n\n</Tab>\n\n\n<Tab label=\"Download\">\n\n<PageDescription>\n\nObtain the Terraform infrastructure-as-code (IasC) scripts that will install the tools into your IBM Cloud managed cluster\n\n</PageDescription>\n\n- Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem\n    ```bash\n    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git\n    ```\n\n- Switch to the cloned directory\n    ```bash\n    cd ibm-garage-iteration-zero\n    ```\n\n</Tab>\n\n<Tab label=\"API key\">\n\n<PageDescription>\n\nConfigure the key the CLI uses to create cloud services\n\n</PageDescription>\n\nThe installation scripts need an API key to authenticate to IBM Cloud.\nThe following steps create the required API key and put them into a properties file.\n\n- The `credentials.properties` property file looks like this:\n\n    ```bash\n    classic.username=<CLASSIC_USERNAME>\n    classic.api.key=<CLASSIC_API_KEY>\n    ibmcloud.api.key=<IBMCLOUD_API_KEY>\n    ```\n\nSince the cluster already exists, we'll only need to set the `ibmcloud.api.key` property.\n\n### Step A. Create the credentials.properties file\n\n- Inside the `ibm-garage-iteration-zero` folder, copy `credentials.template` to a file named `credentials.properties`.\n\n  ```bash\n  cd ibm-garage-iteration-zero\n  cp credentials.template credentials.properties\n  ```\n\n  The `credentials.properties` file is already listed in the `.gitignore` file so that the file containing\nyour private API key will not be pushed to the server repository.\n\n### Step B. Set the IBM Cloud API key\n\n- Follow these instructions to create an IBM Cloud API key:\n\n    - [Managing user API keys](https://cloud.ibm.com/docs/iam?topic=iam-userapikey \"Managing user API keys\")\n\n- Edit the `credentials.properties` file to set the value for the `ibmcloud.api.key` property.\n\n</Tab>\n\n<Tab label=\"Configuration\">\n\n<PageDescription>\n\nConfigure the properties describing the environment\n\n</PageDescription>\n\nThe settings for installing the Developer Tools go in a single property file\nin the `./terraform/settings` directory:\n- `environment.tfvars` -- Properties for installing the Developer Tools\n\nFollow the instructions below to set the existing cluster's details in the `environment.tfvars` file.\n\n### Environment variables\n\nThe `environment.tfvars` properties will be used to install the Development Tools.\n\n- Typical values look like this:\n\n    ```bash\n    # The type of cluster that will be created/used (kubernetes or openshift)\n    cluster_type=\"openshift\"\n    # Flag indicating if we are using an existing cluster or creating a new one\n    cluster_exists=\"true\"\n\n    # The prefix that should be applied to the cluster name and service names (if not provided\n    # explicitly). If not provided then the resource_group_name will be used as the prefix.\n    #name_prefix=\"<name prefix for cluster and services>\"\n    #name_prefix=\"garage-cloud-surge\"\n\n    # The cluster name can be provided (particularly if using an existing cluster). The value\n    # for cluster name used by the scripts will be set in the following order of presidence:\n    # - \"${cluster_name}\"\n    # - \"${name_prefix}-cluster\"\n    # - \"${resource_group_name}-cluster\"\n    #cluster_name=\"<cluster name>\"\n    cluster_name=\"garage-cloud-surge-ocp-cluster\"\n\n    resource_group_name=\"garage-cloud-surge\"\n    vlan_region=\"us-south\"\n    ```\n\n<p/>\n\nSet them based on the existing cluster:\n- `resource_group_name` -- The existing cluster's resource group\n- `vlan_region` -- The existing cluster's region\n- `cluster_exists` -- Set to `true` for an existing cluster\n- `cluster_type` -- Specify the existing cluster's type\n    - **kubernetes** -- Kubernetes\n    - **openshift** -- OpenShift v3\n    - **ocp3** -- OpenShift v3\n    - **ocp4** -- OpenShift v4\n    - **crc** -- CodeReady Containers\n- `cluster_name` -- The existing cluster's name\n- `registry_namespace` -- The namespace that will be created with the IBM Container Registry. If not provided\nthe value will default to the `resource_group_name`\n- `logdna_exists` -- \"true\" or \"false\" flag indicating that the logdna instance already exists and a new one \nshould not be provisioned\n- `logdna_name` -- The name of the logdna instance, particularly if one already exists. If not provided the name\nwill be generated as `{prefix-name || resource-group-name}-logdna`\n- `sysdig_exists` -- \"true\" or \"false\" flag indicating that the sysdig instance already exists and a new one \nshould not be provisioned\n- `sysdig_name` -- The name of the sysdig instance, particularly if one already exists. If not provided the name\nwill be generated as `{prefix-name || resource-group-name}-sysdig`\n\n<p/>\n\n<InlineNotification>\n\n**Note**: The values for `resource_group_name` and `cluster_name` can be found on the Resource List\npage in the IBM Cloud Console - https://cloud.ibm.com/resources\n\n</InlineNotification>\n\n<p/>\n\n![Resource list](/images/cloud.ibm.com-resources.png)\n\n<p/>\n\n</Tab>\n\n<Tab label=\"Run\">\n\n<PageDescription>\n\nRun the scripts that create the environment\n\n</PageDescription>\n\nHaving configured the `credentials.properties` and `environment.tfvars` properties files, we are now ready to kick off the installation.\n\nLaunch a [Developer Tools Docker container](https://github.com/ibm-garage-cloud/ibm-garage-cli-tools \"Cloud Garage Tools Docker image\").\n\n- Run the following command to run the Docker container:\n    ```bash\n    ./launch.sh\n    ```\n\n-  This will install the Cloud Garage Tools Docker image and exec shell into the running container. The container will\n    mount the filesystem's `./terraform/` directory as `/home/devops/src/`. Once the Docker container has started and\n    the script has exec shelled into it, you will see an IBM Garage banner. This will help you identify you are running\n    inside the Docker image that has just mounted your file system.\n\n    For more information on the **Developer Tools Image** see the following guide link below.\n\n- Create the <Globals name=\"env\" />\n\n    The supplied Terraform script is ready to run using the settings in the properties files.\n    You optionally can extend or modify the scripts and tailor them for your project's specific needs.\n\n<InlineNotification>\n\n**Note**: If you run this approach multiple times remember to delete any pre existing cloud services that were created previously\n\n</InlineNotification>\n\n- Run this script in the container:\n    ```bash\n    ./runTerraform.sh\n    ```\n\n    This script will create the <Globals name=\"env\" />.\n\n    If you executed the script previously for the current cluster configuration and the workspace directory still\n    exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if\n    you want to use the state from the previous run as a starting point to either add or remove configuration. Delete\n    the workspace if you want to start with a clean install of the Toolkit.\n\n    The script will verify some basic settings and prompt if you want to proceed. After you select **Y** (for yes), \n    the Terraform Apply process will begin to create the infrastructure and services for your environment.\n    \n    Both of the prompts can be skipped via command-line arguments passed to the script. To bypass the workspace\n    prompt, provided either `--delete` or `--keep` to either delete or keep the workspace, respectively. To\n    bypass the verification prompt provide `--auto-approve` and the script will automatically answer `Yes` to\n    the prompt and start the terraform process.\n\n    Installing the tools into an existing cluster takes about 30 minutes.\n\n    <InlineNotification kind=\"success\">\n\n    You should now have your <Globals name=\"env\" />\n        fully provisioned and configured. Enjoy!\n\n    </InlineNotification>\n\n### <Globals name=\"env\" />\n\nOnce the Terraform scripts have finished, you can see the resources that the scripts created in IBM Cloud:\n- In the IBM Cloud console, open the [Resource List](https://cloud.ibm.com/docs/overview?topic=overview-ui#dashboardview \"Managing resources in the resource list\")\n- On the Resource List page, filter by your Resource Group (e.g. `appdev-team`)\n- You should see these resources listed:\n    - **Clusters**: 1, either Kubernetes or OpenShift\n    - **Services**: 5 or so, such as PostgreSQL, LogDNA, etc.\n    - **Storage**: 1, an instance of Cloud Object Storage\n- Select the cluster and open the Kubernetes dashboard or OpenShift web console. You should see:\n    - Namespaces: `tools`, `dev`, `test`, and `staging`\n    - Deployments in the `tools` namespace: `developer-dashboard`, `jenkins`, etc.\n\nTo get started with code use the following guides:\n\n<AnchorLinks>\n  <AnchorLink to=\"/getting-started-day-1/deploy-app\">Deploy First App</AnchorLink>\n  <AnchorLink to=\"/tools\">Guides</AnchorLink>\n  <AnchorLink to=\"/tools/tools-image\">Developer Tools Image</AnchorLink>\n</AnchorLinks>\n\n### Possible issues\n\nIf you find that that the Terraform provisioning has failed, try re-running the `runTerraform.sh` script again.\nThe state will be saved and Terraform will try and apply the configuration to match the desired end state.\n\nIf you find that some of the services have failed to create in the time allocated, you can try this:\n- Manually delete the service instances in your resource group\n- Delete the `workspace` directory (which will remove any state that has been created by Terraform)\n- Re-run the `runTerraform.sh` script\n    ```bash\n    rm -rf workspace\n    ./runTerraform.sh\n    ```\n\n</Tab>\n\n</Tabs>\n\n","fileAbsolutePath":"/home/runner/work/ibm-garage-developer-guide/ibm-garage-developer-guide/src/pages/admin/installation-existing/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","3273249464","63531786","63531786","768070550"]}