{"componentChunkName":"component---src-pages-admin-config-install-index-mdx","path":"/admin/config-install/","result":{"pageContext":{"frontmatter":{"title":"Configure Environment"},"relativePagePath":"/admin/config-install/index.mdx","titleType":"page","MdxNode":{"id":"286a037f-57e6-57a1-8b7d-28f142da7e8f","children":[],"parent":"50a70937-f2a3-59ea-80a0-86ef0d93c051","internal":{"content":"---\ntitle: Configure Environment\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nConfigure the toolkit development environment after installation\n\n</PageDescription>\n\n<InlineNotification>\n\n**Note**: An **environment administrator** performs the steps on this page. See [Plan Installation > Roles](/admin/plan-installation#roles) for the overview of the roles involved.\n\n</InlineNotification>\n\n### Post installation Tools setup\n\nThe following post installation setup is required to get the development\n enabled quickly, make sure you have completed at least post installation\n  tasks. The customization is optional and down to development team needs.\n\n- Apply the `RBAC` Rules to development cluster, this restricts the behaviour\n of the development team. Given them access to create content but not\n  manipulate the lower level configuration of the development cluster. \n\n- Run the RBAC script `./terraform/scripts/rbac.sh {ACCESS-GROUP}`\n    - `{ACCESS-GROUP}` is the name of the admin group (i.e. `{resource_group\n    }-ADMIN`)\n\n- Check you log data if flowing into [LogDNA](/guides/log-management)\n- Complete the setup of [Sysdig](/guides/monitoring#sysdig-dashboard) to check the monitoring data\n is flowing\n- Managing development assets is an important part of any **SDLC**, The open\n source version of Artifactory has been installed into the cluster, This\n  enables the full end to end process SDLC to be demostrated. This\n   version requires some manual configuration after its installation. These\n    these instructions can be found here [Artifactory Setup](/admin/artifactory-setup)\n\n- Complete the [Argo CD Setup](continuous-delivery#configuring-gitops-with\n-argo-cd), this configures ArgoCD to use Artifactory as a Helm Repository\n- Test opening the [<Globals name=\"dashboard\" />](/getting-started/dashboard)\n    - Run either or all of the CLI options to load the dashboard `oc dashboard\n     | kubectl dashboard | igc dashboard`\n- Test the pipeline by [deploying a first app](/getting-started/deploy-app)\n- [Set up a GitOps repo](/practical/inventory-part2#using-cd-to-deploy-to-test) to validate ArgoCD setup and configuration\n \n### Post installation Customization \n\nWith the <Globals name=\"shortName\" /> now installed into your development\n cluster. It is now possible to customize it further to meet your development\n  teams requirements. This maybe to use it for a small squad that just has\n   ten developers, or prepare it for a large activation workshop tht could\n    have two hundred developers. The following customization helps enable the\n     control you need for the developers.\n\n\n- You can customize tools that are displayed in the <Globals name=\"dashboard\" /> follow these instructions [Customize Dashboard](dashboard#customizing-the-tools-view), the following is an common set that help a developer get quickly to the resourced they need to be productive.\n    ```\n    igc tool-config --name github --url {url to git org}\n    igc tool-config --name ir --url {url image registry}\n    igc tool-config --name logdna --url {url to LogDNA instance}\n    igc tool-config --name sysdig --url {url to Sysdig instance}\n    igc tool-config --name ta --url {url to the Transformation Advisor}\n    igc tool-config --name codeready --url {url to the CRW instance}\n    igc tool-config --name mcm --url {url to IBM CP4MCM}\n    igc tool-config --name integration --url {url to CP4I instance}  \n    ```\n  \n- Generated new passwords for **SonarQube** and update the secret in the\n `tool` namespace\n- Test all the installed tools with new passwords\n- Test end to end flow for an application and validate the content in each tool\n\n## Optional Tool Configuration\n\nThese are optional steps post installation. They do make help with an\n enhanced developer experience.\n\n### Adding Tools to OpenShift Console\n\nYou can customize the OpenShift Console. You can extend the tools menu and\n provide fast links to common tools you the development team will require. These tools link are common across the cluster.\n\n![OCP Console Tools](./ocp-console-tools.png)\n\n- From the `terraform/scripts` folded edit the file called `tools.yaml` this\n file contains the CRDs required to configure the menus. You should look\n  through this and add custom links for `github`, `logdna` and `sysdig` save\n   the file. \n\n- Make sure you are logged into your cluster from the command line and run\n the following script. You will need the ingress sub domain for your cluster. This can be found on the cluster overview in IBM Cloud.\n    ```\n    ./config-console-tools {cluster ingress subdomain}\n    ```\n    \n- You can optionaly extend the list of tools using more of the supported\n tools, here is an example extending links to the Cloud Pak for Integration\n  and Cloud Pak for Multi Cloud Management.\n    ```\n    ---\n    apiVersion: console.openshift.io/v1\n    kind: ConsoleLink\n    metadata:\n      name: toolkit-mcm\n    spec:\n      applicationMenu:\n        imageURL: https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/mcm\n        section: Cloud Native Toolkit\n      href: https://icp-console.gsi-learning-ocp43-7ec5d722a0ab3f463fdc90eeb94dbc70-0000.us-east.containers.appdomain.cloud/\n      location: ApplicationMenu\n      text: Multi Cloud Manager\n    ---\n    apiVersion: console.openshift.io/v1\n    kind: ConsoleLink\n    metadata:\n      name: toolkit-integration\n    spec:\n      applicationMenu:\n        imageURL: https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/integration\n        section: Cloud Native Toolkit\n      href: https://navigator-integration.gsi-ocp311-integration-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.us-east.containers.appdomain.cloud/\n      location: ApplicationMenu\n      text: Integration\n    ```\n\n### CodeReady Workspace Installation\n\nThe following tools can be easily added to your development experience using\n the Red Hat Operator Hub.\n\nCodeReady Workspaces is a developer workspace server and cloud IDE\n. Workspaces are defined as project code files and all of their dependencies\n necessary to edit, build, run, and debug them. Each workspace has its own\n  private IDE hosted within it. The IDE is accessible through a browser. The\n   browser downloads the IDE as a single-page web application. CodeReady\n    Workspaces will enable a 100% developer experience to be delivered from a\n     users browser. This is perfect for running enablement learning from\n      constrained developer laptops, or for SREs to make quick change to a\n       microservice.\n       \n### CodeReady Workspaces Intallation Pre-requisite\n\nProvision  the OpenShift 4.3 Cluster Ensure the logged in user has the\n Administrator Privileges Ensure you have created a new Project to manage the\n  \"codeready workspace operator & cluster \"\n\nSetting up the CRW Operator & Cluster:\n- Navigate to Operator Hub and search of the Red Hat Cloudready workspace,\n Click on the operator and select the appropriate workspace to install the CRW operator. \n- Navigate to the installed operator to view the CRW operator. \n- Now click the link visible as part of the operator to create/view the\n CheCluster part of the workspace\n- Create the CheCluster button, to navigate to the YAML Configuration\n page (displays all the parameters). \n- You need to change the following parameter mentioned below, As part of\n the storage section, please add the following parameters\n    ```    \n    postgresPVCStorageClassName: ibmc-block-gold\n    workspacePVCStorageClassName: ibmc-block-gold\n    ```\n\n- Post definition the yaml section for storage will look as below\n    ```\n     :storage:\n        postgresPVCStorageClassName: ibmc-block-gold\n        pvcStrategy: per-workspace\n        pvcClaimSize: 1Gi\n        preCreateSubPaths: true\n        workspacePVCStorageClassName: ibmc-block-gold\n    ```\n\n- Now create, the cluster after doing the above changes. The cluster will take few minutes to get created as its resources such as Postgres DB, Keycloak Auth, CRW workspace will get created.\n- Once Cluster is created navigate to the overview tab of the CheCluster in\n the CRW operator. You will be able to see the below :\n    - URL of the CodeReady Workspaces URL\n    - URL of the Red Hat SSO Admin Console URL \n    - oAuth SSO Enabled.\n - This should be enabled by default, if not please slide the button to\n  enable and confirm\n - TLS Would be disabled. Please slide the button to enable https connectivity\n to the CRW workspace and confirm\n - You have now completed the provisioning of the Code Ready Workspaces\n  operator into your development cluster and will enable it to be used by the\n   developers that plan to use this development cluster","type":"Mdx","contentDigest":"553ded93998e3a3a2c389bfdbaab0731","counter":496,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Configure Environment"},"exports":{},"rawBody":"---\ntitle: Configure Environment\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nConfigure the toolkit development environment after installation\n\n</PageDescription>\n\n<InlineNotification>\n\n**Note**: An **environment administrator** performs the steps on this page. See [Plan Installation > Roles](/admin/plan-installation#roles) for the overview of the roles involved.\n\n</InlineNotification>\n\n### Post installation Tools setup\n\nThe following post installation setup is required to get the development\n enabled quickly, make sure you have completed at least post installation\n  tasks. The customization is optional and down to development team needs.\n\n- Apply the `RBAC` Rules to development cluster, this restricts the behaviour\n of the development team. Given them access to create content but not\n  manipulate the lower level configuration of the development cluster. \n\n- Run the RBAC script `./terraform/scripts/rbac.sh {ACCESS-GROUP}`\n    - `{ACCESS-GROUP}` is the name of the admin group (i.e. `{resource_group\n    }-ADMIN`)\n\n- Check you log data if flowing into [LogDNA](/guides/log-management)\n- Complete the setup of [Sysdig](/guides/monitoring#sysdig-dashboard) to check the monitoring data\n is flowing\n- Managing development assets is an important part of any **SDLC**, The open\n source version of Artifactory has been installed into the cluster, This\n  enables the full end to end process SDLC to be demostrated. This\n   version requires some manual configuration after its installation. These\n    these instructions can be found here [Artifactory Setup](/admin/artifactory-setup)\n\n- Complete the [Argo CD Setup](continuous-delivery#configuring-gitops-with\n-argo-cd), this configures ArgoCD to use Artifactory as a Helm Repository\n- Test opening the [<Globals name=\"dashboard\" />](/getting-started/dashboard)\n    - Run either or all of the CLI options to load the dashboard `oc dashboard\n     | kubectl dashboard | igc dashboard`\n- Test the pipeline by [deploying a first app](/getting-started/deploy-app)\n- [Set up a GitOps repo](/practical/inventory-part2#using-cd-to-deploy-to-test) to validate ArgoCD setup and configuration\n \n### Post installation Customization \n\nWith the <Globals name=\"shortName\" /> now installed into your development\n cluster. It is now possible to customize it further to meet your development\n  teams requirements. This maybe to use it for a small squad that just has\n   ten developers, or prepare it for a large activation workshop tht could\n    have two hundred developers. The following customization helps enable the\n     control you need for the developers.\n\n\n- You can customize tools that are displayed in the <Globals name=\"dashboard\" /> follow these instructions [Customize Dashboard](dashboard#customizing-the-tools-view), the following is an common set that help a developer get quickly to the resourced they need to be productive.\n    ```\n    igc tool-config --name github --url {url to git org}\n    igc tool-config --name ir --url {url image registry}\n    igc tool-config --name logdna --url {url to LogDNA instance}\n    igc tool-config --name sysdig --url {url to Sysdig instance}\n    igc tool-config --name ta --url {url to the Transformation Advisor}\n    igc tool-config --name codeready --url {url to the CRW instance}\n    igc tool-config --name mcm --url {url to IBM CP4MCM}\n    igc tool-config --name integration --url {url to CP4I instance}  \n    ```\n  \n- Generated new passwords for **SonarQube** and update the secret in the\n `tool` namespace\n- Test all the installed tools with new passwords\n- Test end to end flow for an application and validate the content in each tool\n\n## Optional Tool Configuration\n\nThese are optional steps post installation. They do make help with an\n enhanced developer experience.\n\n### Adding Tools to OpenShift Console\n\nYou can customize the OpenShift Console. You can extend the tools menu and\n provide fast links to common tools you the development team will require. These tools link are common across the cluster.\n\n![OCP Console Tools](./ocp-console-tools.png)\n\n- From the `terraform/scripts` folded edit the file called `tools.yaml` this\n file contains the CRDs required to configure the menus. You should look\n  through this and add custom links for `github`, `logdna` and `sysdig` save\n   the file. \n\n- Make sure you are logged into your cluster from the command line and run\n the following script. You will need the ingress sub domain for your cluster. This can be found on the cluster overview in IBM Cloud.\n    ```\n    ./config-console-tools {cluster ingress subdomain}\n    ```\n    \n- You can optionaly extend the list of tools using more of the supported\n tools, here is an example extending links to the Cloud Pak for Integration\n  and Cloud Pak for Multi Cloud Management.\n    ```\n    ---\n    apiVersion: console.openshift.io/v1\n    kind: ConsoleLink\n    metadata:\n      name: toolkit-mcm\n    spec:\n      applicationMenu:\n        imageURL: https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/mcm\n        section: Cloud Native Toolkit\n      href: https://icp-console.gsi-learning-ocp43-7ec5d722a0ab3f463fdc90eeb94dbc70-0000.us-east.containers.appdomain.cloud/\n      location: ApplicationMenu\n      text: Multi Cloud Manager\n    ---\n    apiVersion: console.openshift.io/v1\n    kind: ConsoleLink\n    metadata:\n      name: toolkit-integration\n    spec:\n      applicationMenu:\n        imageURL: https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/integration\n        section: Cloud Native Toolkit\n      href: https://navigator-integration.gsi-ocp311-integration-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.us-east.containers.appdomain.cloud/\n      location: ApplicationMenu\n      text: Integration\n    ```\n\n### CodeReady Workspace Installation\n\nThe following tools can be easily added to your development experience using\n the Red Hat Operator Hub.\n\nCodeReady Workspaces is a developer workspace server and cloud IDE\n. Workspaces are defined as project code files and all of their dependencies\n necessary to edit, build, run, and debug them. Each workspace has its own\n  private IDE hosted within it. The IDE is accessible through a browser. The\n   browser downloads the IDE as a single-page web application. CodeReady\n    Workspaces will enable a 100% developer experience to be delivered from a\n     users browser. This is perfect for running enablement learning from\n      constrained developer laptops, or for SREs to make quick change to a\n       microservice.\n       \n### CodeReady Workspaces Intallation Pre-requisite\n\nProvision  the OpenShift 4.3 Cluster Ensure the logged in user has the\n Administrator Privileges Ensure you have created a new Project to manage the\n  \"codeready workspace operator & cluster \"\n\nSetting up the CRW Operator & Cluster:\n- Navigate to Operator Hub and search of the Red Hat Cloudready workspace,\n Click on the operator and select the appropriate workspace to install the CRW operator. \n- Navigate to the installed operator to view the CRW operator. \n- Now click the link visible as part of the operator to create/view the\n CheCluster part of the workspace\n- Create the CheCluster button, to navigate to the YAML Configuration\n page (displays all the parameters). \n- You need to change the following parameter mentioned below, As part of\n the storage section, please add the following parameters\n    ```    \n    postgresPVCStorageClassName: ibmc-block-gold\n    workspacePVCStorageClassName: ibmc-block-gold\n    ```\n\n- Post definition the yaml section for storage will look as below\n    ```\n     :storage:\n        postgresPVCStorageClassName: ibmc-block-gold\n        pvcStrategy: per-workspace\n        pvcClaimSize: 1Gi\n        preCreateSubPaths: true\n        workspacePVCStorageClassName: ibmc-block-gold\n    ```\n\n- Now create, the cluster after doing the above changes. The cluster will take few minutes to get created as its resources such as Postgres DB, Keycloak Auth, CRW workspace will get created.\n- Once Cluster is created navigate to the overview tab of the CheCluster in\n the CRW operator. You will be able to see the below :\n    - URL of the CodeReady Workspaces URL\n    - URL of the Red Hat SSO Admin Console URL \n    - oAuth SSO Enabled.\n - This should be enabled by default, if not please slide the button to\n  enable and confirm\n - TLS Would be disabled. Please slide the button to enable https connectivity\n to the CRW workspace and confirm\n - You have now completed the provisioning of the Code Ready Workspaces\n  operator into your development cluster and will enable it to be used by the\n   developers that plan to use this development cluster","fileAbsolutePath":"/home/travis/build/ibm-garage-cloud/ibm-garage-developer-guide/src/pages/admin/config-install/index.mdx"}}}}